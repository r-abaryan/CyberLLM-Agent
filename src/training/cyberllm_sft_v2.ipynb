{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CyberLLM SFT V2 - Continual Fine-tuning\n",
        "\n",
        "Fine-tuning on new structured cybersecurity attack dataset\n",
        "\n",
        "## Optimized for Agentic Workflow:\n",
        "- **Example 1**: Threat Assessment & Identification (assess, identify, severity)\n",
        "- **Example 2**: Detection, Response & Prevention (detect, respond, recover, prevent)\n",
        "\n",
        "Creates 2 focused training examples per CSV row (~200 examples from 100 rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PREVIOUS_MODEL_PATH = \"./cyberllm_sft_model\"\n",
        "CSV_DATASET_PATH = \"./dataset/cybersecurity_attacks.csv\"\n",
        "OUTPUT_DIR = \"./cyberllm_sft_v2\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CyberLLM SFT V2 Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CyberLLMSFT_V2:\n",
        "    def __init__(self, model_path=\"./cyberllm_sft_model\", output_dir=\"./cyberllm_sft_model_v2\"):\n",
        "        self.model_path = model_path\n",
        "        self.output_dir = output_dir\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        \n",
        "        self.SYSTEM_PROMPT = \"\"\"You are a cybersecurity expert providing detailed technical analysis and actionable guidance. Structure your response with clear answers and reasoning.\"\"\"\n",
        "    \n",
        "    def load_model_and_tokenizer(self, use_quantization=False):\n",
        "        logger.info(f\"Loading model from: {self.model_path}\")\n",
        "        \n",
        "        quantization_config = None\n",
        "        if use_quantization:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True, \n",
        "                bnb_4bit_compute_dtype=torch.float16\n",
        "            )\n",
        "        \n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_path, \n",
        "            use_fast=True, \n",
        "            padding_side=\"left\"\n",
        "        )\n",
        "        \n",
        "        # Load model from checkpoint\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_path,\n",
        "            quantization_config=quantization_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        \n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        logger.info(\"Model and tokenizer loaded successfully\")\n",
        "    \n",
        "    def load_csv_dataset(self, csv_path, test_size=0.1):\n",
        "        logger.info(f\"Loading dataset from: {csv_path}\")\n",
        "        \n",
        "        df = pd.read_csv(csv_path)\n",
        "        logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
        "        logger.info(f\"Columns: {df.columns.tolist()}\")\n",
        "        \n",
        "        # Shuffle and split\n",
        "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "        split_idx = int(len(df) * (1 - test_size))\n",
        "        \n",
        "        train_df = df[:split_idx]\n",
        "        val_df = df[split_idx:]\n",
        "        \n",
        "        logger.info(f\"Train samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "        \n",
        "        return train_df, val_df\n",
        "    \n",
        "    def create_training_examples(self, row):\n",
        "        examples = []\n",
        "        \n",
        "        # Clean up text fields\n",
        "        def clean_text(text):\n",
        "            if pd.isna(text):\n",
        "                return \"\"\n",
        "            return str(text).strip()\n",
        "        \n",
        "        title = clean_text(row.get('Title', ''))\n",
        "        category = clean_text(row.get('Category', ''))\n",
        "        attack_type = clean_text(row.get('Attack Type', ''))\n",
        "        scenario = clean_text(row.get('Scenario Description', ''))\n",
        "        tools = clean_text(row.get('Tools Used', ''))\n",
        "        steps = clean_text(row.get('Attack Steps ', '') or row.get('Attack Steps', ''))\n",
        "        target = clean_text(row.get('Target Type', ''))\n",
        "        vulnerability = clean_text(row.get('Vulnerability', ''))\n",
        "        mitre = clean_text(row.get('MITRE Technique', ''))\n",
        "        impact = clean_text(row.get('Impact', ''))\n",
        "        detection = clean_text(row.get('Detection Method', ''))\n",
        "        solution = clean_text(row.get('Solution', ''))\n",
        "        \n",
        "        # Example 1: Threat Assessment & Identification\n",
        "        if attack_type and scenario and impact:\n",
        "            question1 = f\"Assess this threat: {title}\"\n",
        "            answer1 = f\"\"\"**Threat Identification:**\n",
        "Attack Type: {attack_type}\n",
        "Category: {category}\n",
        "Target: {target}\n",
        "\n",
        "**Scenario:**\n",
        "{scenario}\n",
        "\n",
        "**Attack Method:**\n",
        "{steps}\n",
        "\n",
        "**Tools Used:** {tools}\n",
        "\n",
        "**Severity/Impact:**\n",
        "{impact}\n",
        "\n",
        "**Exploited Vulnerability:**\n",
        "{vulnerability}\n",
        "\n",
        "**MITRE ATT&CK:** {mitre}\"\"\"\n",
        "            \n",
        "            reasoning1 = f\"This assessment identifies {attack_type} in {category}, analyzes the attack scenario, execution method, severity impact, and maps to MITRE framework for complete threat intelligence.\"\n",
        "            \n",
        "            examples.append((question1, answer1, reasoning1))\n",
        "        \n",
        "        # Example 2: Detection, Response & Prevention\n",
        "        if detection and solution:\n",
        "            question2 = f\"How do I detect, respond to, and prevent {attack_type}?\"\n",
        "            answer2 = f\"\"\"**Detection:**\n",
        "{detection}\n",
        "\n",
        "**Immediate Response & Recovery:**\n",
        "{solution}\n",
        "\n",
        "**Prevention:**\n",
        "Address the root vulnerability: {vulnerability}\n",
        "Implement continuous monitoring and security controls to prevent recurrence.\n",
        "\n",
        "**MITRE Monitoring:** {mitre}\"\"\"\n",
        "            \n",
        "            reasoning2 = f\"This response provides complete incident handling for {attack_type}: detection methods, response and recovery procedures from the solution data, and prevention strategies targeting the root vulnerability.\"\n",
        "            \n",
        "            examples.append((question2, answer2, reasoning2))\n",
        "        \n",
        "        return examples\n",
        "    \n",
        "    def prepare_sft_dataset(self, df, num_samples=None):\n",
        "        system_prompt = self.SYSTEM_PROMPT.strip()\n",
        "        \n",
        "        if num_samples:\n",
        "            df = df.head(num_samples)\n",
        "        \n",
        "        texts = []\n",
        "        \n",
        "        for idx, row in df.iterrows():\n",
        "            # Create multiple Q&A pairs from each row\n",
        "            examples = self.create_training_examples(row)\n",
        "            \n",
        "            for question, answer, reasoning in examples:\n",
        "                if question and answer:  # Only add if both exist\n",
        "                    messages = [\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": question},\n",
        "                        {\"role\": \"assistant\", \"content\": f\"<answer>\\n{answer}\\n</answer>\\n<reasoning>\\n{reasoning}\\n</reasoning>\"}\n",
        "                    ]\n",
        "                    \n",
        "                    text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "                    texts.append(text)\n",
        "        \n",
        "        logger.info(f\"Created {len(texts)} training examples from {len(df)} CSV rows\")\n",
        "        \n",
        "        # Create dataset\n",
        "        formatted_dataset = Dataset.from_dict({\"text\": texts})\n",
        "        return formatted_dataset\n",
        "    \n",
        "    def train(self, train_dataset, eval_dataset, epochs=3, learning_rate=1e-5, batch_size=12):\n",
        "        logger.info(\"Starting continual fine-tuning...\")\n",
        "        \n",
        "        training_args = SFTConfig(\n",
        "            output_dir=self.output_dir,\n",
        "            learning_rate=learning_rate,\n",
        "            eval_steps=100,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=1000,\n",
        "            logging_steps=10,\n",
        "            num_train_epochs=epochs,\n",
        "            warmup_ratio=0.05,  # Lower warmup for continual learning\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            gradient_accumulation_steps=1,\n",
        "            weight_decay=0.01,\n",
        "            max_grad_norm=0.3,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            bf16=True,\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=1,\n",
        "            max_seq_length=768,  # Longer sequences for detailed attack scenarios\n",
        "            packing=False,\n",
        "            # eval_strategy=\"steps\",\n",
        "        )\n",
        "        \n",
        "        self.trainer = SFTTrainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "        )\n",
        "        \n",
        "        logger.info(\"Training started...\")\n",
        "        self.trainer.train()\n",
        "        \n",
        "        logger.info(\"Saving model...\")\n",
        "        self.trainer.save_model(self.output_dir)\n",
        "        self.tokenizer.save_pretrained(self.output_dir)\n",
        "        \n",
        "        logger.info(f\"Training completed! Model saved to {self.output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize and Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not Path(CSV_DATASET_PATH).exists():\n",
        "    print(f\"Error: CSV file not found: {CSV_DATASET_PATH}\")\n",
        "else:\n",
        "    print(f\"CSV file found: {CSV_DATASET_PATH}\")\n",
        "\n",
        "trainer = CyberLLMSFT_V2(\n",
        "    model_path=PREVIOUS_MODEL_PATH,\n",
        "    output_dir=OUTPUT_DIR\n",
        ")\n",
        "\n",
        "print(f\"\\nLoading previous model from: {PREVIOUS_MODEL_PATH}\")\n",
        "trainer.load_model_and_tokenizer(use_quantization=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loading CSV dataset from: {CSV_DATASET_PATH}\")\n",
        "train_df, val_df = trainer.load_csv_dataset(CSV_DATASET_PATH, test_size=0.1)\n",
        "\n",
        "print(\"\\nPreparing training datasets...\")\n",
        "train_dataset = trainer.prepare_sft_dataset(train_df)\n",
        "eval_dataset = trainer.prepare_sft_dataset(val_df)\n",
        "\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"  Training examples: {len(train_dataset)}\")\n",
        "print(f\"  Validation examples: {len(eval_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nStarting continual fine-tuning...\")\n",
        "print(\"Using lower learning rate (1e-5) to preserve previous knowledge\")\n",
        "\n",
        "trainer.train(\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    epochs=3,  # Fewer epochs for continual learning\n",
        "    learning_rate=1e-5,  # Lower LR to avoid catastrophic forgetting\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "print(f\"\\nContinual training completed\")\n",
        "print(f\"Updated model saved to: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Model (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_question = \"How does SQL injection work?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": trainer.SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": test_question}\n",
        "]\n",
        "\n",
        "inputs = trainer.tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True\n",
        ").to(trainer.model.device)\n",
        "\n",
        "outputs = trainer.model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "response = trainer.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Question: {test_question}\\n\")\n",
        "print(f\"Response:\\n{response}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
