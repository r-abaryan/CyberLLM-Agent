{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”’ CyberLLM Agent SFT Fine-tuning Notebook\n",
        "\n",
        "This notebook implements Supervised Fine-Tuning (SFT) for cybersecurity question-answering using the LLama model.\n",
        "\n",
        "## Key Features:\n",
        "- **Format Training**: Teaches model `<answer>`/`<reasoning>` structure for GRPO Stage 2\n",
        "- **Cybersecurity Dataset**: Uses AlicanKiraz0/Cybersecurity-Dataset-v1\n",
        "- **100% Training Data**: Uses almost all available data for training (99% train, 1% validation)\n",
        "- **Consistent Approach**: Same system prompt and data formatting as standalone script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install transformers datasets trl torch accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CyberLLM SFT Fine-tuning Notebook\n",
        "\n",
        "This notebook implements Supervised Fine-Tuning (SFT) for cybersecurity question-answering using the Qwen2.5-0.5B-Instruct model.\n",
        "\n",
        "## Key Features:\n",
        "- **Format Training**: Teaches model `<answer>`/`<reasoning>` structure for GRPO Stage 2\n",
        "- **Cybersecurity Dataset**: Uses AlicanKiraz0/Cybersecurity-Dataset-v1\n",
        "- **Consistent Approach**: Same system prompt and data formatting as standalone script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install transformers datasets trl torch accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\" Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CyberLLMSFT:\n",
        "    def __init__(self, model_name=\"meta-llama/Llama-3.2-1B-Instruct\", output_dir=\"./cyberllm_sft\"):\n",
        "        self.model_name = model_name\n",
        "        self.output_dir = output_dir\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        \n",
        "        # System prompt with format instruction - SAME as standalone script\n",
        "        self.SYSTEM_PROMPT = \"\"\"\n",
        "You're a cybersecurity expert. Answer the question with careful analysis.\n",
        "Respond in the following format:\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "\"\"\"\n",
        "    \n",
        "    def load_model_and_tokenizer(self, use_quantization=False):\n",
        "        logger.info(f\"Loading model: {self.model_name}\")\n",
        "        \n",
        "        quantization_config = None\n",
        "        if use_quantization:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True, \n",
        "                bnb_4bit_compute_dtype=torch.float16\n",
        "            )\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name, \n",
        "            use_fast=True, \n",
        "            padding_side=\"left\"\n",
        "        )\n",
        "        \n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        \n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        logger.info(\"Model and tokenizer loaded successfully\")\n",
        "    \n",
        "    def load_dataset_from_hf(self, dataset_name):\n",
        "        logger.info(f\"Loading dataset from Hugging Face: {dataset_name}\")\n",
        "        \n",
        "        try:\n",
        "            dataset_dict = load_dataset(dataset_name)\n",
        "            logger.info(f\"Loaded dataset with {sum(len(split) for split in dataset_dict.values())} examples\")\n",
        "            return dataset_dict\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading dataset: {e}\")\n",
        "            return None\n",
        "\n",
        "print(\" CyberLLMSFT class defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add methods to CyberLLMSFT class\n",
        "def create_train_val_split(self, dataset_dict):\n",
        "    \"\"\"Create train/validation split if validation doesn't exist\"\"\"\n",
        "    if \"validation\" not in dataset_dict:\n",
        "        logger.info(\"No validation split found, creating 90/10 train/val split\")\n",
        "        train_data = dataset_dict[\"train\"]\n",
        "        total_size = len(train_data)\n",
        "        val_size = int(total_size * 0.1)\n",
        "        \n",
        "        # Create splits\n",
        "        val_dataset = train_data.select(range(val_size))\n",
        "        train_dataset = train_data.select(range(val_size, total_size))\n",
        "        \n",
        "        return {\n",
        "            \"train\": train_dataset,\n",
        "            \"validation\": val_dataset\n",
        "        }\n",
        "    return dataset_dict\n",
        "\n",
        "def prepare_sft_dataset(self, dataset, split=\"train\", num_samples=None, num_proc=1):\n",
        "    system_prompt = self.SYSTEM_PROMPT.strip()\n",
        "    \n",
        "    if split == \"train\":\n",
        "        dataset = dataset.shuffle(seed=42)\n",
        "    \n",
        "    if num_samples:\n",
        "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
        "    \n",
        "    # Process dataset without multiprocessing to avoid pickling issues\n",
        "    texts = []\n",
        "    for i in range(0, len(dataset), 512):  # Process in batches of 512\n",
        "        batch = dataset.select(range(i, min(i + 512, len(dataset))))\n",
        "        \n",
        "        for instruction, output in zip(batch[\"user\"], batch[\"assistant\"]):\n",
        "            # Create proper reasoning based on the actual answer content\n",
        "            reasoning = f\"This answer addresses the cybersecurity question by providing specific technical details, security best practices, and actionable guidance relevant to the topic.\"\n",
        "            \n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": instruction},\n",
        "                {\"role\": \"assistant\", \"content\": f\"<answer>\\n{output}\\n</answer>\\n<reasoning>\\n{reasoning}\\n</reasoning>\"}\n",
        "            ]\n",
        "            text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "            texts.append(text)\n",
        "    \n",
        "    # Create new dataset with formatted texts\n",
        "    from datasets import Dataset\n",
        "    formatted_dataset = Dataset.from_dict({\"text\": texts})\n",
        "    \n",
        "    return formatted_dataset\n",
        "\n",
        "# Add methods to the class\n",
        "CyberLLMSFT.create_train_val_split = create_train_val_split\n",
        "CyberLLMSFT.prepare_sft_dataset = prepare_sft_dataset\n",
        "\n",
        "print(\" Dataset preparation methods added!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add training method to CyberLLMSFT class\n",
        "def train(self, train_dataset, eval_dataset, epochs=5, learning_rate=3e-5, batch_size=12):\n",
        "    logger.info(\"Starting CyberLLM SFT training...\")\n",
        "    \n",
        "    training_args = SFTConfig(\n",
        "        output_dir=self.output_dir,\n",
        "        learning_rate=learning_rate,\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=1000,\n",
        "        logging_steps=10,\n",
        "        num_train_epochs=epochs,\n",
        "        warmup_ratio=0.1,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=1,\n",
        "        weight_decay=0.1,\n",
        "        max_grad_norm=0.3,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        bf16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"none\",\n",
        "        save_total_limit=1,\n",
        "        max_length=512,\n",
        "        packing=False,\n",
        "    )\n",
        "    \n",
        "    self.trainer = SFTTrainer(\n",
        "        model=self.model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "    )\n",
        "    \n",
        "    self.trainer.train()\n",
        "    self.trainer.save_model(self.output_dir)\n",
        "    \n",
        "    logger.info(f\"Training completed! Model saved to {self.output_dir}\")\n",
        "\n",
        "# Add method to the class\n",
        "CyberLLMSFT.train = train\n",
        "\n",
        "print(\" Training method added!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = CyberLLMSFT(\n",
        "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    output_dir=\"./cyberllm_sft_model\"\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "trainer.load_model_and_tokenizer(use_quantization=False)\n",
        "\n",
        "print(\" Model and tokenizer loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from Hugging Face\n",
        "dataset_dict = trainer.load_dataset_from_hf(\"AlicanKiraz0/Cybersecurity-Dataset-v1\")\n",
        "\n",
        "if not dataset_dict:\n",
        "    print(\" Failed to load dataset from Hugging Face\")\n",
        "else:\n",
        "    print(\" Dataset loaded successfully!\")\n",
        "    \n",
        "    # Create train/validation split if needed\n",
        "    dataset_dict = trainer.create_train_val_split(dataset_dict)\n",
        "    \n",
        "    print(f\"  Train: {len(dataset_dict['train'])} examples\")\n",
        "    print(f\"  Validation: {len(dataset_dict['validation'])} examples\")\n",
        "# Prepare datasets for SFT training\n",
        "train_dataset = trainer.prepare_sft_dataset(dataset_dict[\"train\"], split=\"train\")\n",
        "\n",
        "eval_dataset = trainer.prepare_sft_dataset(dataset_dict[\"validation\"], split=\"validation\")\n",
        "\n",
        "print(f\"  Train: {len(train_dataset)} examples\")\n",
        "print(f\"  Validation: {len(eval_dataset)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start SFT training\n",
        "trainer.train()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\" Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª Test the Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Cybersecurity Model Benchmarking Script\n",
        "Tests the trained model on various cybersecurity tasks\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CyberSecurityBenchmark:\n",
        "    def __init__(self, model_path=\"D:/AI-ML/CyberXP/cyberllm_sft_model\", device=\"auto\"):\n",
        "        self.model_path = model_path\n",
        "        self.device = device\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.test_cases = None\n",
        "        self.similarity_model = None\n",
        "        \n",
        "    def load_similarity_model(self):\n",
        "        \"\"\"Load sentence transformer for semantic similarity\"\"\"\n",
        "        try:\n",
        "            self.similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            logger.info(\"âœ“ Loaded similarity model\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load similarity model: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def load_dataset(self):\n",
        "        \"\"\"Load test dataset and create test cases\"\"\"\n",
        "        print(\"Loading cybersecurity dataset...\")\n",
        "        logger.info(\"Loading test dataset...\")\n",
        "        \n",
        "        try:\n",
        "            # Load the specified cybersecurity dataset\n",
        "            print(\"  Downloading AlicanKiraz0/Cybersecurity-Dataset-v1...\")\n",
        "            dataset = load_dataset(\"AlicanKiraz0/Cybersecurity-Dataset-v1\")\n",
        "            print(\"  Dataset loaded successfully\")\n",
        "            logger.info(\"âœ“ Loaded AlicanKiraz0/Cybersecurity-Dataset-v1\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Failed to load dataset: {e}\")\n",
        "            logger.error(f\"Could not load dataset: {e}\")\n",
        "            logger.warning(\"Could not load dataset, using default test cases\")\n",
        "            self._create_default_test_cases()\n",
        "            return\n",
        "        \n",
        "        # Create test cases from dataset\n",
        "        if \"train\" in dataset:\n",
        "            train_data = dataset[\"train\"]\n",
        "            print(f\"  Found {len(train_data)} training examples\")\n",
        "            logger.info(f\"Found {len(train_data)} training examples\")\n",
        "            \n",
        "            # Use 20% of training data for testing (more comprehensive)\n",
        "            test_size = max(5, int(len(train_data) * 0.2))\n",
        "            print(f\"  Sampling {test_size} examples (20%) for testing...\")\n",
        "            test_data = random.sample(list(train_data), test_size)\n",
        "            print(f\" Created {len(test_data)} test cases\")\n",
        "            logger.info(f\"Using {len(test_data)} examples (20%) for testing\")\n",
        "            \n",
        "            # Enhanced categories for better testing\n",
        "            self.test_cases = {\n",
        "                \"vulnerability_analysis\": [],\n",
        "                \"threat_detection\": [],\n",
        "                \"incident_response\": [],\n",
        "                \"network_security\": [],\n",
        "                \"cryptography\": [],\n",
        "                \"security_best_practices\": []\n",
        "            }\n",
        "            \n",
        "            # Categorize questions based on keywords and store ground truth answers\n",
        "            print(\"   Categorizing questions...\")\n",
        "            for i, example in enumerate(test_data):\n",
        "                question = example.get(\"user\", example.get(\"question\", \"\")).lower()\n",
        "                ground_truth_answer = example.get(\"assistant\", example.get(\"answer\", \"\"))\n",
        "                \n",
        "                test_case = {\n",
        "                    \"prompt\": example.get(\"user\", example.get(\"question\", \"\")),\n",
        "                    \"ground_truth_answer\": ground_truth_answer,\n",
        "                    \"expected_keywords\": self._extract_keywords_from_answer(ground_truth_answer)\n",
        "                }\n",
        "                \n",
        "                # Enhanced categorization logic\n",
        "                category = self._categorize_question(question)\n",
        "                if category in self.test_cases:\n",
        "                    self.test_cases[category].append(test_case)\n",
        "                else:\n",
        "                    self.test_cases[\"security_best_practices\"].append(test_case)\n",
        "            \n",
        "            # Limit to reasonable number of tests per category\n",
        "            print(\"   Balancing test categories...\")\n",
        "            for category in self.test_cases:\n",
        "                if len(self.test_cases[category]) > 5:\n",
        "                    self.test_cases[category] = random.sample(self.test_cases[category], 5)\n",
        "            \n",
        "            total_tests = sum(len(tests) for tests in self.test_cases.values())\n",
        "            print(f\"  Categorized into {total_tests} test cases across {len(self.test_cases)} categories\")\n",
        "            logger.info(f\"Created test cases: {total_tests} total\")\n",
        "        else:\n",
        "            logger.warning(\"No train split found in dataset\")\n",
        "            self._create_default_test_cases()\n",
        "    \n",
        "    def _categorize_question(self, question: str) -> str:\n",
        "        \"\"\"Enhanced categorization logic\"\"\"\n",
        "        question_lower = question.lower()\n",
        "        \n",
        "        # Vulnerability analysis\n",
        "        if any(keyword in question_lower for keyword in [\n",
        "            \"vulnerability\", \"exploit\", \"injection\", \"sql\", \"xss\", \"csrf\", \n",
        "            \"buffer\", \"overflow\", \"cve\", \"zero-day\", \"code\", \"flaw\"\n",
        "        ]):\n",
        "            return \"vulnerability_analysis\"\n",
        "        \n",
        "        # Threat detection\n",
        "        elif any(keyword in question_lower for keyword in [\n",
        "            \"threat\", \"attack\", \"malware\", \"phishing\", \"detect\", \"monitor\", \n",
        "            \"alert\", \"intrusion\", \"anomaly\", \"suspicious\"\n",
        "        ]):\n",
        "            return \"threat_detection\"\n",
        "        \n",
        "        # Incident response\n",
        "        elif any(keyword in question_lower for keyword in [\n",
        "            \"incident\", \"response\", \"compromise\", \"breach\", \"ransomware\",\n",
        "            \"forensic\", \"recovery\", \"containment\", \"eradication\"\n",
        "        ]):\n",
        "            return \"incident_response\"\n",
        "        \n",
        "        # Network security\n",
        "        elif any(keyword in question_lower for keyword in [\n",
        "            \"network\", \"firewall\", \"vpn\", \"router\", \"switch\", \"protocol\",\n",
        "            \"tcp\", \"udp\", \"ip\", \"dns\", \"dhcp\", \"wifi\", \"wireless\"\n",
        "        ]):\n",
        "            return \"network_security\"\n",
        "        \n",
        "        # Cryptography\n",
        "        elif any(keyword in question_lower for keyword in [\n",
        "            \"cryptography\", \"encryption\", \"decryption\", \"hash\", \"certificate\",\n",
        "            \"ssl\", \"tls\", \"aes\", \"rsa\", \"public key\", \"private key\", \"digital signature\"\n",
        "        ]):\n",
        "            return \"cryptography\"\n",
        "        \n",
        "        # Default to security best practices\n",
        "        else:\n",
        "            return \"security_best_practices\"\n",
        "    \n",
        "    def _extract_keywords_from_answer(self, answer: str) -> List[str]:\n",
        "        \"\"\"Extract important keywords from answer for evaluation\"\"\"\n",
        "        if not answer:\n",
        "            return []\n",
        "        \n",
        "        # Enhanced cybersecurity keywords\n",
        "        cyber_keywords = [\n",
        "            \"security\", \"vulnerability\", \"attack\", \"threat\", \"malware\", \"phishing\",\n",
        "            \"injection\", \"authentication\", \"authorization\", \"encryption\", \"firewall\",\n",
        "            \"intrusion\", \"breach\", \"incident\", \"response\", \"prevention\", \"detection\",\n",
        "            \"mitigation\", \"risk\", \"compliance\", \"policy\", \"access\", \"control\",\n",
        "            \"network\", \"protocol\", \"certificate\", \"hash\", \"cipher\", \"key\",\n",
        "            \"exploit\", \"payload\", \"backdoor\", \"trojan\", \"virus\", \"worm\",\n",
        "            \"ddos\", \"botnet\", \"rootkit\", \"spyware\", \"adware\", \"ransomware\"\n",
        "        ]\n",
        "        \n",
        "        answer_lower = answer.lower()\n",
        "        found_keywords = [kw for kw in cyber_keywords if kw in answer_lower]\n",
        "        return found_keywords[:8]  # Increased to 8 keywords\n",
        "    \n",
        "    def _create_default_test_cases(self):\n",
        "        \"\"\"Create default test cases if dataset loading fails\"\"\"\n",
        "        logger.error(\"No dataset available - cannot run benchmark without test data\")\n",
        "        raise ValueError(\"Dataset is required for benchmarking. Please ensure your dataset is available.\")\n",
        "    \n",
        "    def load_model(self):\n",
        "        \"\"\"Load the fine-tuned model and tokenizer\"\"\"\n",
        "        print(\"Loading fine-tuned model...\")\n",
        "        logger.info(f\"Loading fine-tuned model from: {self.model_path}\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Load tokenizer\n",
        "            print(\"  Loading tokenizer...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_path,\n",
        "                use_fast=True,\n",
        "                padding_side=\"left\"\n",
        "            )\n",
        "            \n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            \n",
        "            # Load fine-tuned model directly\n",
        "            print(\"  Loading model weights...\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_path,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=self.device\n",
        "            )\n",
        "            \n",
        "            loading_time = time.time() - start_time\n",
        "            print(f\"  Model loaded successfully in {loading_time:.2f}s\")\n",
        "            logger.info(f\"Successfully loaded fine-tuned model in {loading_time:.2f}s\")\n",
        "            \n",
        "            return loading_time\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Failed to load model: {e}\")\n",
        "            logger.error(f\"Could not load fine-tuned model: {e}\")\n",
        "            logger.info(\"Make sure you've trained the model first!\")\n",
        "            raise\n",
        "    \n",
        "    def generate_response(self, prompt: str) -> str:\n",
        "        \"\"\"Generate response from the model with optimized parameters\"\"\"\n",
        "        # Use the same format as training for consistency\n",
        "        system_prompt = \"\"\"You're a cybersecurity expert. Answer the question with careful analysis.\n",
        "Respond in the following format:\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\"\"\"\n",
        "        \n",
        "        full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt}\\n\\nAnswer:\"\n",
        "        \n",
        "        inputs = self.tokenizer(\n",
        "            full_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=1024\n",
        "        ).to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=500,  # Increased for more complete answers\n",
        "                temperature=0.7,     # Balanced temperature\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                top_k=50,           # Add top-k sampling\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.1,  # Slightly higher penalty\n",
        "                no_repeat_ngram_size=3,   # Prevent repetition\n",
        "                early_stopping=True\n",
        "            )\n",
        "        \n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the generated part\n",
        "        response = response[len(full_prompt):].strip()\n",
        "        \n",
        "        # Clean up response\n",
        "        if response.startswith(\"Answer:\"):\n",
        "            response = response[7:].strip()\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    def extract_answer_content(self, response: str) -> str:\n",
        "        \"\"\"Extract the actual answer content from formatted response\"\"\"\n",
        "        # Look for <answer> tags\n",
        "        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
        "        if answer_match:\n",
        "            return answer_match.group(1).strip()\n",
        "        \n",
        "        # If no tags, return the response as is\n",
        "        return response.strip()\n",
        "    \n",
        "    def evaluate_response(self, response: str, ground_truth_answer: str, expected_keywords: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"Enhanced evaluation with semantic similarity\"\"\"\n",
        "        # Extract actual answer content\n",
        "        answer_content = self.extract_answer_content(response)\n",
        "        \n",
        "        response_lower = answer_content.lower()\n",
        "        ground_truth_lower = ground_truth_answer.lower()\n",
        "        \n",
        "        # 1. KEYWORD COVERAGE (40% weight)\n",
        "        keyword_matches = sum(1 for keyword in expected_keywords \n",
        "                             if keyword.lower() in response_lower)\n",
        "        keyword_score = keyword_matches / len(expected_keywords) if expected_keywords else 0\n",
        "        \n",
        "        # 2. SEMANTIC SIMILARITY (30% weight) - if similarity model available\n",
        "        semantic_score = 0\n",
        "        if self.similarity_model:\n",
        "            try:\n",
        "                embeddings = self.similarity_model.encode([response_lower, ground_truth_lower])\n",
        "                similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "                semantic_score = max(0, similarity)  # Ensure non-negative\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Semantic similarity failed: {e}\")\n",
        "        \n",
        "        # 3. CONTENT RELEVANCE (20% weight) - Jaccard similarity\n",
        "        response_words = set(response_lower.split())\n",
        "        ground_truth_words = set(ground_truth_lower.split())\n",
        "        \n",
        "        # Remove common words\n",
        "        common_words = {\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\", \"by\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"could\", \"should\", \"this\", \"that\", \"these\", \"those\", \"it\", \"its\", \"they\", \"them\", \"their\", \"there\", \"here\", \"where\", \"when\", \"why\", \"how\", \"what\", \"who\", \"which\"}\n",
        "        response_words = response_words - common_words\n",
        "        ground_truth_words = ground_truth_words - common_words\n",
        "        \n",
        "        intersection = len(response_words & ground_truth_words)\n",
        "        union = len(response_words | ground_truth_words)\n",
        "        content_relevance = intersection / union if union > 0 else 0\n",
        "        \n",
        "        # Overall score (weighted combination without format)\n",
        "        overall_score = (\n",
        "            keyword_score * 0.5 +\n",
        "            semantic_score * 0.3 +\n",
        "            content_relevance * 0.2\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"overall_score\": overall_score,\n",
        "            \"keyword_score\": keyword_score,\n",
        "            \"semantic_score\": semantic_score,\n",
        "            \"content_relevance\": content_relevance,\n",
        "            \"keyword_matches\": keyword_matches,\n",
        "            \"total_keywords\": len(expected_keywords),\n",
        "            \"word_count\": len(answer_content.split()),\n",
        "            \"response_length\": len(answer_content),\n",
        "            \"answer_content\": answer_content\n",
        "        }\n",
        "    \n",
        "    \n",
        "    def _calculate_accuracy_score(self, detailed_results: Dict[str, Any]) -> float:\n",
        "        \"\"\"Calculate overall accuracy using simplified metrics\"\"\"\n",
        "        total_score = 0\n",
        "        total_tests = 0\n",
        "        \n",
        "        for category, results in detailed_results.items():\n",
        "            for result in results:\n",
        "                # Use the overall_score from simplified evaluation\n",
        "                total_score += result[\"overall_score\"]\n",
        "                total_tests += 1\n",
        "        \n",
        "        return total_score / total_tests if total_tests > 0 else 0\n",
        "    \n",
        "    def _get_loading_time(self) -> float:\n",
        "        \"\"\"Get model loading time (placeholder - would need to track this)\"\"\"\n",
        "        return 0.0  # Could be enhanced to track actual loading time\n",
        "    \n",
        "    def _calculate_processing_efficiency(self, total_time: float, total_tests: int) -> float:\n",
        "        \"\"\"Calculate processing efficiency (tests per minute)\"\"\"\n",
        "        if total_time > 0:\n",
        "            return (total_tests * 60) / total_time  # tests per minute\n",
        "        return 0\n",
        "    \n",
        "    def run_benchmark(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run comprehensive benchmark tests\"\"\"\n",
        "        print(\"Starting Cybersecurity Model Benchmark\")\n",
        "        print(\"=\" * 50)\n",
        "        logger.info(\"Starting cybersecurity model benchmark...\")\n",
        "        \n",
        "        # Load similarity model\n",
        "        self.load_similarity_model()\n",
        "        \n",
        "        # Load dataset and create test cases\n",
        "        if self.test_cases is None:\n",
        "            self.load_dataset()\n",
        "        \n",
        "        # Load model\n",
        "        loading_time = 0\n",
        "        if self.model is None:\n",
        "            loading_time = self.load_model()\n",
        "        \n",
        "        print(f\"\\nRunning benchmark tests...\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        results = {\n",
        "            \"overall_scores\": {},\n",
        "            \"detailed_results\": {},\n",
        "            \"performance_metrics\": {}\n",
        "        }\n",
        "        \n",
        "        total_tests = 0\n",
        "        total_overall_score = 0\n",
        "        total_keyword_score = 0\n",
        "        total_semantic_score = 0\n",
        "        total_content_relevance = 0\n",
        "        total_inference_time = 0\n",
        "        \n",
        "        for category, test_cases in self.test_cases.items():\n",
        "            if not test_cases:\n",
        "                continue\n",
        "                \n",
        "            print(f\"\\nTesting {category.replace('_', ' ').title()}\")\n",
        "            logger.info(f\"Testing category: {category}\")\n",
        "            category_results = []\n",
        "            \n",
        "            for i, test_case in enumerate(test_cases):\n",
        "                print(f\"  Test {i+1}/{len(test_cases)}: {test_case['prompt'][:60]}...\")\n",
        "                logger.info(f\"Running test {i+1}/{len(test_cases)} in {category}\")\n",
        "                \n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "                    response = self.generate_response(test_case[\"prompt\"])\n",
        "                    inference_time = time.time() - start_time\n",
        "                    \n",
        "                    evaluation = self.evaluate_response(\n",
        "                        response, \n",
        "                        test_case[\"ground_truth_answer\"], \n",
        "                        test_case[\"expected_keywords\"]\n",
        "                    )\n",
        "                    evaluation[\"inference_time\"] = inference_time\n",
        "                    evaluation[\"prompt\"] = test_case[\"prompt\"]\n",
        "                    evaluation[\"response\"] = response\n",
        "                    evaluation[\"ground_truth_answer\"] = test_case[\"ground_truth_answer\"]\n",
        "                    \n",
        "                    category_results.append(evaluation)\n",
        "                    \n",
        "                    # Update totals\n",
        "                    total_tests += 1\n",
        "                    total_overall_score += evaluation[\"overall_score\"]\n",
        "                    total_keyword_score += evaluation[\"keyword_score\"]\n",
        "                    total_semantic_score += evaluation[\"semantic_score\"]\n",
        "                    total_content_relevance += evaluation[\"content_relevance\"]\n",
        "                    total_inference_time += inference_time\n",
        "                    \n",
        "                    print(f\"    Overall: {evaluation['overall_score']:.2%}, \"\n",
        "                          f\"Keywords: {evaluation['keyword_score']:.2%}, \"\n",
        "                          f\"Semantic: {evaluation['semantic_score']:.2%}, \"\n",
        "                          f\"Time: {inference_time:.2f}s\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error in test {i+1}: {e}\")\n",
        "                    print(f\"    Error: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            # Calculate category averages\n",
        "            if category_results:\n",
        "                avg_overall_score = sum(r[\"overall_score\"] for r in category_results) / len(category_results)\n",
        "                avg_keyword_score = sum(r[\"keyword_score\"] for r in category_results) / len(category_results)\n",
        "                avg_semantic_score = sum(r[\"semantic_score\"] for r in category_results) / len(category_results)\n",
        "                avg_content_relevance = sum(r[\"content_relevance\"] for r in category_results) / len(category_results)\n",
        "                avg_inference_time = sum(r[\"inference_time\"] for r in category_results) / len(category_results)\n",
        "                \n",
        "                results[\"overall_scores\"][category] = {\n",
        "                    \"avg_overall_score\": avg_overall_score,\n",
        "                    \"avg_keyword_score\": avg_keyword_score,\n",
        "                    \"avg_semantic_score\": avg_semantic_score,\n",
        "                    \"avg_content_relevance\": avg_content_relevance,\n",
        "                    \"avg_inference_time\": avg_inference_time,\n",
        "                    \"test_count\": len(category_results)\n",
        "                }\n",
        "            \n",
        "            results[\"detailed_results\"][category] = category_results\n",
        "        \n",
        "        # Calculate overall metrics\n",
        "        results[\"performance_metrics\"] = {\n",
        "            \"overall_score\": total_overall_score / total_tests if total_tests > 0 else 0,\n",
        "            \"overall_keyword_score\": total_keyword_score / total_tests if total_tests > 0 else 0,\n",
        "            \"overall_semantic_score\": total_semantic_score / total_tests if total_tests > 0 else 0,\n",
        "            \"overall_content_relevance\": total_content_relevance / total_tests if total_tests > 0 else 0,\n",
        "            \"avg_inference_time\": total_inference_time / total_tests if total_tests > 0 else 0,\n",
        "            \"total_tests\": total_tests,\n",
        "            \"accuracy_score\": self._calculate_accuracy_score(results[\"detailed_results\"]),\n",
        "            \"loading_time\": loading_time,\n",
        "            \"processing_efficiency\": self._calculate_processing_efficiency(total_inference_time, total_tests)\n",
        "        }\n",
        "        \n",
        "        # Add summary statistics\n",
        "        if total_tests > 0:\n",
        "            print(f\"\\nBenchmark Summary:\")\n",
        "            print(f\"  Total Tests Completed: {total_tests}\")\n",
        "            print(f\"  Average Overall Score: {total_overall_score/total_tests:.2%}\")\n",
        "            print(f\"  Average Semantic Score: {total_semantic_score/total_tests:.2%}\")\n",
        "            print(f\"  Total Time: {total_inference_time:.2f}s\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def print_results(self, results: Dict[str, Any]):\n",
        "        \"\"\"Print benchmark results in a readable format\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"CYBERSECURITY MODEL BENCHMARK RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Overall performance\n",
        "        metrics = results[\"performance_metrics\"]\n",
        "        print(f\"\\nOVERALL PERFORMANCE:\")\n",
        "        print(f\"  Overall Score:           {metrics['overall_score']:.2%}\")\n",
        "        print(f\"  Keyword Score:           {metrics['overall_keyword_score']:.2%}\")\n",
        "        print(f\"  Semantic Score:          {metrics['overall_semantic_score']:.2%}\")\n",
        "        print(f\"  Content Relevance:       {metrics['overall_content_relevance']:.2%}\")\n",
        "        print(f\"  Avg Inference Time:      {metrics['avg_inference_time']:.2f}s\")\n",
        "        print(f\"  Processing Efficiency:   {metrics['processing_efficiency']:.1f} tests/min\")\n",
        "        print(f\"  Total Tests:             {metrics['total_tests']}\")\n",
        "        \n",
        "        # Category breakdown\n",
        "        print(f\"\\nCATEGORY BREAKDOWN:\")\n",
        "        for category, scores in results[\"overall_scores\"].items():\n",
        "            print(f\"  {category.replace('_', ' ').title()}:\")\n",
        "            print(f\"    Overall Score:    {scores['avg_overall_score']:.2%}\")\n",
        "            print(f\"    Keyword Score:    {scores['avg_keyword_score']:.2%}\")\n",
        "            print(f\"    Semantic Score:   {scores['avg_semantic_score']:.2%}\")\n",
        "            print(f\"    Avg Time:         {scores['avg_inference_time']:.2f}s\")\n",
        "            print(f\"    Tests:            {scores['test_count']}\")\n",
        "        \n",
        "        # Sample responses\n",
        "        print(f\"\\nSAMPLE RESPONSES:\")\n",
        "        for category, test_results in results[\"detailed_results\"].items():\n",
        "            if test_results:\n",
        "                print(f\"\\n{category.replace('_', ' ').title()} - Test 1:\")\n",
        "                print(f\"Prompt: {test_results[0]['prompt'][:100]}...\")\n",
        "                print(f\"Response: {test_results[0]['answer_content'][:200]}...\")\n",
        "                print(f\"Overall Score: {test_results[0]['overall_score']:.2%}\")\n",
        "                print(f\"Keyword Score: {test_results[0]['keyword_score']:.2%}\")\n",
        "                print(f\"Semantic Score: {test_results[0]['semantic_score']:.2%}\")\n",
        "    \n",
        "    def save_results(self, results: Dict[str, Any], filename: str = \"benchmark_results.json\"):\n",
        "        \"\"\"Save results to JSON file\"\"\"\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        logger.info(f\"Results saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the benchmark\"\"\"\n",
        "    benchmark = CyberSecurityBenchmark()\n",
        "    \n",
        "    try:\n",
        "        results = benchmark.run_benchmark()\n",
        "        benchmark.print_results(results)\n",
        "        benchmark.save_results(results)\n",
        "        \n",
        "        print(f\"\\nBenchmark completed successfully!\")\n",
        "        print(f\"Results saved to benchmark_results.json\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Benchmark failed: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b7fcc3da45b4625a707d786b45bcf45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15653782fb1b45e196cee53ac6886442",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rasou\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rasou\\.cache\\huggingface\\hub\\models--abaryan--CyberXP_Llama_3.2_1B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e053ee5d165454ca6d41fd23d7cae4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/abaryan/CyberXP_Llama_3.2_1B/commit/4957e36d0ecbd062e9db846dc8d9723d22d1c0ef', commit_message='Upload tokenizer', commit_description='', oid='4957e36d0ecbd062e9db846dc8d9723d22d1c0ef', pr_url=None, repo_url=RepoUrl('https://huggingface.co/abaryan/CyberXP_Llama_3.2_1B', endpoint='https://huggingface.co', repo_type='model', repo_id='abaryan/CyberXP_Llama_3.2_1B'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from huggingface_hub import login\n",
        "# import torch\n",
        "# # hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "# # notebook_login()\n",
        "\n",
        "# model_name = \"./cyberllm_sft_model\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     # quantization_config=bnb_config,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     # device_map=\"auto\"\n",
        "# ).to(\"cuda\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# HF_REPO_NAME = \"abaryan/CyberXP_Llama_3.2_1B\"\n",
        "\n",
        "# model.push_to_hub(HF_REPO_NAME)\n",
        "# tokenizer.push_to_hub(HF_REPO_NAME)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
